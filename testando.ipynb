{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.7088652578943647, 0.7697645172444486, 5.2254240521431345, 1.2983454980344349]\n",
      "[(2, 5.2254240521431345), (3, 1.2983454980344349), (1, 0.7697645172444486), (0, 0.7088652578943647)]\n",
      "PI-Explicação: \n",
      "- petal length (cm) - 4.7\n",
      "- petal width (cm) - 1.2\n",
      "- sepal width (cm) - 2.8\n",
      "- sepal length (cm) - 6.1\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names) # organizando em um df\n",
    "df['target'] = iris.target # rotulos\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train) # treinando o modelo\n",
    "\n",
    "\n",
    "w = modelo.coef_[0] # pesos do modelo treinado\n",
    "Vs = X_test.iloc[0].to_dict() # utilizxando a primeira instância dos dados de teste \n",
    "\n",
    "delta = []\n",
    "\n",
    "# Calcula o valor delta para cada feature\n",
    "# peso negativo, valor menor = maior possibilidade da instância peetencer a classe\n",
    "# delta c\\ peso positivo é (valor feature na instancia - valor min da feature no dataset) * o peso da feature \n",
    "# delta c\\ peso negativo é (vvalor max da feature no dataset - valor feature na instancia - ) * o peso da feature\n",
    "for i, feature in enumerate(df.columns[:-1]):  \n",
    "    if w[i] < 0:\n",
    "        delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "    else:\n",
    "        delta.append((Vs[feature] - df[feature].min()) * w[i])\n",
    "\n",
    "# Calcula o limiar (-Gamma_w)\n",
    "\n",
    "R = sum(delta) # \"(Vs predição da regressão loghistica)  # Usando a equação 13 do artigo e garantindo que R seja positivo\n",
    "# R = O valor absoluto da soma de todos os deltas\n",
    "\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    # Ordena o delta junto com seus índices, em ordem decrescente de valor absoluto\n",
    "    print(delta)\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True) # enumerando os valores de delta em tuplas e ordenanas\n",
    "    print(delta_sorted)                                                                               # func lambda recebe a tupla produzida com o valor absoluto em ordem decrescente, do maior para o menor.\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    # Limite para considerar uma feature como importante, com base no limar R\n",
    "    # threshold_delta = 0 * R # Valor de delta mínimo para ser considerado relevante\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]  # Desempacota o índice(soted_idx) e o valor(delta_value) do delta de acordo com o indce(idx)\n",
    "        feature = X_test.columns[sorted_idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação apenas se o delta for maior que o threshold que está sendo calculado com uma % do R. não sei se esta certo isso\n",
    "       # if abs(delta_value) > threshold_delta:  # Verifica se o delta tem impacto relevante \n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice também para o loop\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "# Computa a PI-explicação\n",
    "Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "#print(f\"Pesos: {w}\")\n",
    "#print(f\"Delta: {delta}\")\n",
    "#print(f\"Limiar R: {R:.2f}\")\n",
    "#print(f\"Vs: {Vs}\\n\")\n",
    "print(f\"PI-Explicação: \")\n",
    "for item in Xpl:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OLHAI MAH -    sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "73                6.1               2.8                4.7               1.2\n",
      "1\n",
      "0.8277146135745481\n",
      "Probabilidades: [0.00380009 0.82771461 0.16848529]\n",
      "Classe verdadeira: 1\n",
      "Valor de gamma_A: 0.8277146135745481\n",
      "PI-Explicação: \n",
      "- petal length (cm) - 4.7\n",
      "- petal width (cm) - 1.2\n",
      "- sepal width (cm) - 2.8\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)  # organizando em um DataFrame\n",
    "df['target'] = iris.target  # rótulos\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treina o modelo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train)  # treinando o modelo\n",
    "\n",
    "# Usa a primeira instância dos dados de teste para a explicação\n",
    "Vs = X_test.iloc[0].to_dict()  # utilizando a primeira instância dos dados de teste \n",
    "instancia_test = X_test.iloc[[0]]  # Mantém como DataFrame para preservar os nomes das características\n",
    "\n",
    "\n",
    "\n",
    "# Previsão de probabilidades para a instância\n",
    "# o predict_proba calcula a probabilidade de uma instância pertencer a uma das classes possíveis usando um modelo de classificação treinado.\n",
    "probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "# O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "classe_verdadeira = y_test.iloc[0]  # Obtém a classe verdadeira da instância\n",
    "gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "print(classe_verdadeira)\n",
    "print(gamma_A)\n",
    "# Cálculo do valor delta para cada feature\n",
    "delta = []\n",
    "w = modelo.coef_[0]  # pesos do modelo treinado\n",
    "\n",
    "# Calcula o delta para cada feature\n",
    "for i, feature in enumerate(df.columns[:-1]):\n",
    "    if w[i] < 0:\n",
    "        delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "    else:\n",
    "        delta.append((Vs[feature] - df[feature].min()) * w[i])\n",
    "\n",
    "# Calcula R como a soma dos deltas menos gamma_A\n",
    "R = sum(delta) - gamma_A  # Atualiza R para incluir gamma_A\n",
    "\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    # Ordena o delta junto com seus índices, em ordem decrescente de valor absoluto\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True)  # enumerando os valores de delta em tuplas e ordenando\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]  # Desempacota o índice e o valor do delta\n",
    "        feature = X_test.columns[sorted_idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação\n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "# Computa a PI-explicação\n",
    "Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "# Imprime os resultados\n",
    "print(f\"Probabilidades: {probs}\")\n",
    "print(f\"Classe verdadeira: {classe_verdadeira}\")\n",
    "print(f\"Valor de gamma_A: {gamma_A}\")\n",
    "print(f\"PI-Explicação: \")\n",
    "for item in Xpl:\n",
    "    print(f\"- {item}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
