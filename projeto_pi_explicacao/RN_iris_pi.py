# ðŸ”¹ ImportaÃ§Ã£o das bibliotecas necessÃ¡rias
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier
import pandas as pd
import numpy as np

# ðŸ”¹ Carregar o dataset Iris
data = load_iris()
X = pd.DataFrame(data.data, columns=data.feature_names)
y = data.target  # Classes do dataset
class_names = data.target_names

# ðŸ”¹ Transformar o problema em binÃ¡rio (classe 0 contra todas)
y_binario = [1 if label == 0 else 0 for label in y]

# ðŸ”¹ Dividir os dados em treino e teste
X_train, X_test, y_train, y_test = train_test_split(X, y_binario, test_size=0.2, random_state=42)

# ðŸ”¹ Treinar a Rede Neural (MLP)
mlp = MLPClassifier(hidden_layer_sizes=(10, 5), max_iter=1000, learning_rate_init=0.001, random_state=42)
mlp.fit(X_train, y_train)

print("\nâœ… Treinamento concluÃ­do com sucesso!")

# ðŸ”¹ Escolher uma instÃ¢ncia para testar a PI-explicaÃ§Ã£o
idx = 0  # Teste outras instÃ¢ncias mudando o Ã­ndice (1, 2, 3, etc.)
instancia_test = X_test.iloc[[idx]]
classe_prevista = mlp.predict(instancia_test)[0]

print(f"\nðŸ”¹ Classe prevista para a instÃ¢ncia {idx}: {classe_prevista}")

# ðŸ”¹ Obter os nomes das features
feature_names = X.columns.tolist()

# ðŸ”¹ Obter os valores da instÃ¢ncia testada
Vs = X_test.iloc[idx].to_dict()

# ðŸ”¹ Obter os coeficientes (pesos das features)
# ðŸ”¹ Normalizar os coeficientes da rede
w = mlp.coefs_[0].mean(axis=1)
w = w / np.max(np.abs(w))  # Normaliza os pesos entre -1 e 1

# ðŸ”¹ Calcular os valores de influÃªncia das features
delta = [(Vs[feature] - X[feature].mean()) * w[i] for i, feature in enumerate(feature_names)]

# ðŸ”¹ Ajustar R para evitar valores negativos extremos
R = sum(delta) - mlp.predict_log_proba(instancia_test)[0][1]

# ðŸ”¹ Definir um limiar para excluir deltas insignificantes
limiar_delta = np.percentile(np.abs(delta), 25)  # Exclui os 25% menores deltas

Xpl = []  # Lista de features explicativas
delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True)
R_atual = R
Idx = 0

while R_atual >= 0 and Idx < len(delta_sorted):
    sorted_idx, delta_value = delta_sorted[Idx]
    feature = feature_names[sorted_idx]
    feature_value = Vs[feature]

    if abs(delta_value) < limiar_delta:  # Remove valores irrelevantes
        break

    Xpl.append(f"{feature} - {feature_value}")
    R_atual -= delta_value
    Idx += 1

# ðŸ”¹ Exibir a PI-explicaÃ§Ã£o
print("\nðŸ”¹ PI-ExplicaÃ§Ã£o para a Rede Neural:")
for item in Xpl:
    print(f"âœ… {item}")
