{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI-explicação Log-Linear para a instância [6.  2.7 5.1 1.6]: [('petal length (cm)', 5.1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregando e preparando o dataset Iris com duas classes\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_data['class'] = iris.target\n",
    "iris_data = iris_data[iris_data['class'].isin([0, 1])]  # Mantém apenas as classes 0 e 1\n",
    "\n",
    "# Separando os dados de treino e teste\n",
    "X = iris_data[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "y = iris_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinando o modelo de Regressão Logística\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def calculate_pi_explanation_loglinear(instance, clf, features):\n",
    "    \"\"\"Calcula uma PI-explicação de menor tamanho para uma instância utilizando um algoritmo log-linear.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja calcular a PI-explicação (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista de pares valor-atributo que constituem a PI-explicação.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcula a probabilidade da classe prevista\n",
    "    predicted_class = clf.predict(instance.reshape(1, -1))[0]\n",
    "    prob_predicted_class = clf.predict_proba(instance.reshape(1, -1))[0][predicted_class]\n",
    "\n",
    "    # Calcula a probabilidade da outra classe\n",
    "    prob_other_class = clf.predict_proba(instance.reshape(1, -1))[0][1 - predicted_class]\n",
    "\n",
    "    # Calcula a diferença de probabilidade para cada atributo\n",
    "    delta_features = [(feature,\n",
    "                       abs(clf.coef_[0][i] * instance[i]))\n",
    "                      for i, feature in enumerate(features)]\n",
    "\n",
    "    # Ordena os atributos pela diferença de probabilidade (maior primeiro)\n",
    "    delta_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Seleciona os atributos que contribuem para a classe prevista\n",
    "    pi_explanation = []\n",
    "    threshold = prob_predicted_class - prob_other_class\n",
    "    current_threshold = 0\n",
    "    for idx, (feature, delta) in enumerate(delta_features): \n",
    "        current_threshold += delta\n",
    "        pi_explanation.append((feature, instance[features.index(feature)]))\n",
    "        if current_threshold > threshold:\n",
    "            break\n",
    "\n",
    "    return pi_explanation\n",
    "\n",
    "# Selecionando a primeira instância do conjunto de teste como um array NumPy\n",
    "instance = X_test.iloc[0].to_numpy()\n",
    "\n",
    "# Calculando a PI-explicação para a instância\n",
    "pi_explanation = calculate_pi_explanation_loglinear(instance, clf, list(X.columns))\n",
    "\n",
    "# Exibindo a PI-explicação\n",
    "print(f\"PI-explicação Log-Linear para a instância {instance}: {pi_explanation}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6.0: Comprimento da sépala (sepal length) em centímetros.\n",
    "2.7: Largura da sépala (sepal width) em centímetros.\n",
    "5.1: Comprimento da pétala (petal length) em centímetros.\n",
    "1.6: Largura da pétala (petal width) em centímetros.\n",
    "\n",
    "A PI-explicação sugere que o comprimento da pétala é o fator mais influente para a decisão do modelo sobre essa instância específica com 5.1 cm de comprimento.\n",
    "Esse tipo de explicação ajuda a entender quais características são mais importantes para o modelo ao fazer uma previsão específica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI-explicação Log-Linear para a instância [6.3 2.8 5.1 1.5]: [('petal length (cm)', 5.1)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carregando e preparando o dataset Iris com duas classes\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_data['class'] = iris.target\n",
    "iris_data = iris_data[iris_data['class'].isin([1, 2])]  # Mudança para classes 1 e 2\n",
    "\n",
    "# Separando os dados de treino e teste\n",
    "X = iris_data[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "y = iris_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinando o modelo de Regressão Logística\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def calculate_pi_explanation_loglinear(instance, clf, features):\n",
    "    \"\"\"Calcula uma PI-explicação de menor tamanho para uma instância utilizando um algoritmo log-linear.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja calcular a PI-explicação (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista de pares valor-atributo que constituem a PI-explicação.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcula a probabilidade das duas classes previstas\n",
    "    prob_predicted_class = clf.predict_proba(instance.reshape(1, -1))[0]\n",
    "\n",
    "    # Calcula a diferença de probabilidade para cada atributo\n",
    "    delta_features = []\n",
    "    for i, feature in enumerate(features):\n",
    "        coef = clf.coef_[0][i]  # Coeficiente do modelo para o atributo atual\n",
    "        value = instance[i]     # Valor do atributo na instância atual\n",
    "        delta = abs(coef * value)\n",
    "        delta_features.append((feature, delta))\n",
    "\n",
    "\n",
    "    # Ordena os atributos pela diferença de probabilidade (maior primeiro)\n",
    "    delta_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Seleciona os atributos que contribuem para a classe prevista\n",
    "    pi_explanation = []\n",
    "    threshold = abs(prob_predicted_class[0] - prob_predicted_class[1])  # Diferença de probabilidade entre as duas classes\n",
    "    current_threshold = 0\n",
    "    for idx, (feature, delta) in enumerate(delta_features): \n",
    "        current_threshold += delta\n",
    "        pi_explanation.append((feature, instance[features.index(feature)]))\n",
    "        if current_threshold > threshold:\n",
    "            break\n",
    "\n",
    "    return pi_explanation\n",
    "\n",
    "# Selecionando a primeira instância do conjunto de teste como um array NumPy\n",
    "instance = X_test.iloc[0].to_numpy()\n",
    "\n",
    "# Calculando a PI-explicação para a instância\n",
    "pi_explanation = calculate_pi_explanation_loglinear(instance, clf, list(X.columns))\n",
    "\n",
    "# Exibindo a PI-explicação\n",
    "print(f\"PI-explicação Log-Linear para a instância {instance}: {pi_explanation}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n",
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:493: UserWarning: X does not have valid feature names, but LogisticRegression was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[15], line 168\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;66;03m# Calculando o score de cada explicação mínima\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m explanation \u001b[38;5;129;01min\u001b[39;00m minimal_explanations:\n\u001b[1;32m--> 168\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mcalculate_explanation_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43minstance\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexplanation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExplicação: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexplanation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Score: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mscore\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    171\u001b[0m \u001b[38;5;66;03m# Encontrando os atributos que não estão em nenhuma das explicações mínimas\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[15], line 123\u001b[0m, in \u001b[0;36mcalculate_explanation_score\u001b[1;34m(instance, clf, explanation, features)\u001b[0m\n\u001b[0;32m    120\u001b[0m probabilities \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict_proba(instance\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    122\u001b[0m \u001b[38;5;66;03m# Use the predicted class index to select the corresponding probability \u001b[39;00m\n\u001b[1;32m--> 123\u001b[0m prob_without_explanation \u001b[38;5;241m=\u001b[39m \u001b[43mprobabilities\u001b[49m\u001b[43m[\u001b[49m\u001b[43mpredicted_class\u001b[49m\u001b[43m]\u001b[49m \n\u001b[0;32m    125\u001b[0m \u001b[38;5;66;03m# Cria um novo conjunto de dados com apenas os atributos da explicação\u001b[39;00m\n\u001b[0;32m    126\u001b[0m explanation_data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame([instance], columns\u001b[38;5;241m=\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "\u001b[1;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from itertools import combinations\n",
    "\n",
    "# Carregando e preparando o dataset Iris com duas classes\n",
    "iris = load_iris()\n",
    "iris_data = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "iris_data['class'] = iris.target\n",
    "iris_data = iris_data[iris_data['class'].isin([2, 0])]  # Mudança para classes 1 e 2\n",
    "\n",
    "# Separando os dados de treino e teste\n",
    "X = iris_data[['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']]\n",
    "y = iris_data['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Treinando o modelo de Regressão Logística\n",
    "clf = LogisticRegression(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "def calculate_pi_explanation_loglinear(instance, clf, features):\n",
    "    \"\"\"Calcula uma PI-explicação de menor tamanho para uma instância utilizando um algoritmo log-linear.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja calcular a PI-explicação (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista de pares valor-atributo que constituem a PI-explicação.\n",
    "    \"\"\"\n",
    "\n",
    "    # Calcula a probabilidade das duas classes previstas\n",
    "    prob_predicted_class = clf.predict_proba(instance.reshape(1, -1))[0]\n",
    "\n",
    "    # Calcula a diferença de probabilidade para cada atributo\n",
    "    delta_features = []\n",
    "    for i, feature in enumerate(features):\n",
    "        coef = clf.coef_[0][i]  # Coeficiente do modelo para o atributo atual\n",
    "        value = instance[i]     # Valor do atributo na instância atual\n",
    "        delta = abs(coef * value)\n",
    "        delta_features.append((feature, delta))\n",
    "\n",
    "    # Ordena os atributos pela diferença de probabilidade (maior primeiro)\n",
    "    delta_features.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # Seleciona os atributos que contribuem para a classe prevista\n",
    "    pi_explanation = []\n",
    "    threshold = abs(prob_predicted_class[0] - prob_predicted_class[1])  # Diferença de probabilidade entre as duas classes\n",
    "    current_threshold = 0\n",
    "    for idx, (feature, delta) in enumerate(delta_features): \n",
    "        current_threshold += delta\n",
    "        pi_explanation.append((feature, instance[features.index(feature)]))\n",
    "        if current_threshold > threshold:\n",
    "            break\n",
    "\n",
    "    return pi_explanation\n",
    "\n",
    "def generate_all_minimal_explanations(instance, clf, features):\n",
    "    \"\"\"Gera todas as PI-explicações mínimas para uma instância.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja calcular as PI-explicações (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista de listas de pares valor-atributo, representando as PI-explicações mínimas.\n",
    "    \"\"\"\n",
    "\n",
    "    minimal_explanations = []\n",
    "    for i in range(1, len(features) + 1):\n",
    "        for combination in combinations(features, i):\n",
    "            explanation = [(feature, instance[features.index(feature)]) for feature in combination]\n",
    "            if is_explanation_valid(instance, clf, explanation):\n",
    "                minimal_explanations.append(explanation)\n",
    "    return minimal_explanations\n",
    "\n",
    "def is_explanation_valid(instance, clf, explanation):\n",
    "    \"\"\"Verifica se uma explicação é válida, ou seja, se ela contribui para a classe prevista.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja verificar a explicação (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        explanation: A explicação a ser verificada (lista de pares valor-atributo).\n",
    "\n",
    "    Returns:\n",
    "        True se a explicação é válida, False caso contrário.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a copy of X_train and fill in the values from the instance\n",
    "    explanation_data = X_train.copy()\n",
    "    for feature, value in explanation:\n",
    "        explanation_data[feature] = value\n",
    "\n",
    "    # Predict using the updated explanation_data\n",
    "    predicted_class = clf.predict(explanation_data)[0]\n",
    "\n",
    "    # Check if the predicted class matches the original class\n",
    "    return predicted_class == clf.predict(instance.reshape(1, -1))[0]\n",
    "\n",
    "def calculate_explanation_score(instance, clf, explanation, features):\n",
    "    \"\"\"Calcula o score de uma explicação.\n",
    "\n",
    "    Args:\n",
    "        instance: A instância para a qual se deseja calcular o score (array NumPy de uma única linha).\n",
    "        clf: O classificador de Regressão Logística treinado.\n",
    "        explanation: A explicação para a qual se deseja calcular o score (lista de pares valor-atributo).\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        O score da explicação.\n",
    "    \"\"\"\n",
    "\n",
    "    # Predict the class for the instance\n",
    "    predicted_class = clf.predict(instance.reshape(1, -1))[0]\n",
    "\n",
    "    # Get the probabilities for the instance\n",
    "    probabilities = clf.predict_proba(instance.reshape(1, -1))[0]\n",
    "\n",
    "    # Use the predicted class index to select the corresponding probability \n",
    "    prob_without_explanation = probabilities[predicted_class] \n",
    "\n",
    "    # Cria um novo conjunto de dados com apenas os atributos da explicação\n",
    "    explanation_data = pd.DataFrame([instance], columns=X.columns)\n",
    "    explanation_data = explanation_data[[feature for feature, _ in explanation]]\n",
    "\n",
    "    # Calcula a probabilidade da classe prevista com a explicação\n",
    "    prob_with_explanation = clf.predict_proba(explanation_data)[0][predicted_class]  # Corrected line\n",
    "\n",
    "    # O score é a diferença entre as probabilidades\n",
    "    score = prob_with_explanation - prob_without_explanation\n",
    "    return score\n",
    "\n",
    "def find_features_not_in_any_explanation(minimal_explanations, features):\n",
    "    \"\"\"Encontra os atributos que não estão em nenhuma das explicações mínimas.\n",
    "\n",
    "    Args:\n",
    "        minimal_explanations: Uma lista de listas de pares valor-atributo, representando as PI-explicações mínimas.\n",
    "        features: Nomes dos atributos do conjunto de dados.\n",
    "\n",
    "    Returns:\n",
    "        Uma lista de atributos que não estão em nenhuma das explicações mínimas.\n",
    "    \"\"\"\n",
    "\n",
    "    features_in_explanations = set()\n",
    "    for explanation in minimal_explanations:\n",
    "        for feature, _ in explanation:\n",
    "            features_in_explanations.add(feature)\n",
    "\n",
    "    features_not_in_explanations = [feature for feature in features if feature not in features_in_explanations]\n",
    "    return features_not_in_explanations\n",
    "\n",
    "\n",
    "# Selecionando a primeira instância do conjunto de teste como um array NumPy\n",
    "instance = X_test.iloc[0].to_numpy()\n",
    "features = list(X.columns)\n",
    "\n",
    "# Calculando a PI-explicação para a instância\n",
    "pi_explanation = calculate_pi_explanation_loglinear(instance, clf, features)\n",
    "\n",
    "# Gerando todas as explicações mínimas\n",
    "minimal_explanations = generate_all_minimal_explanations(instance, clf, features)\n",
    "\n",
    "# Calculando o score de cada explicação mínima\n",
    "for explanation in minimal_explanations:\n",
    "    score = calculate_explanation_score(instance, clf, explanation, features)\n",
    "    print(f\"Explicação: {explanation}, Score: {score}\")\n",
    "\n",
    "# Encontrando os atributos que não estão em nenhuma das explicações mínimas\n",
    "features_not_in_explanations = find_features_not_in_any_explanation(minimal_explanations, features)\n",
    "print(f\"\\nAtributos não presentes em nenhuma explicação: {features_not_in_explanations}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
