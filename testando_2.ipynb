{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.07876280643270722\n",
      "4.3 - 7.7 *-0.3938140321635358\n",
      "-0.0\n",
      "1.0 - 6.9 *-2.375192750974152\n",
      "0.1997454612360671\n",
      "0.1 - 2.3 *-0.9987273061803346\n",
      "\n",
      "Instância 2:\n",
      "Classe verdadeira: 2 (virginica)\n",
      "Probabilidades: [8.84412614e-09 1.54875125e-03 9.98451240e-01]\n",
      "Valor de gamma_A: 0.9984512399032031\n",
      "PI-Explicação: \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)  # organizando em um DataFrame\n",
    "df['target'] = iris.target  # rótulos\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treina o modelo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train)  # treinando o modelo\n",
    "\n",
    "# Função para analisar instâncias\n",
    "def analisar_instancias(instancia_para_analisar=2):\n",
    "    num_instancias = len(X_test)\n",
    "    \n",
    "    # Se nenhuma instância for especificada, analisa todas as instâncias\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "    \n",
    "    # Loop para analisar instâncias\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X_test.iloc[idx].to_dict()  # Utilizando a instância de teste especificada\n",
    "        instancia_test = X_test.iloc[[idx]]  # Mantém como DataFrame para preservar os nomes das características\n",
    "\n",
    "        # Previsão de probabilidades para a instância\n",
    "        probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "        # O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "        classe_verdadeira = y_test.iloc[idx]  # Obtém a classe verdadeira da instância\n",
    "        gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "        \n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]  # pesos do modelo treinado\n",
    "\n",
    "        # Calcula o delta para cada feature\n",
    "        for i, feature in enumerate(df.columns[:-1]):\n",
    "            if w[i] < 0:\n",
    "                delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "                print((Vs[feature] - df[feature].max()) * w[i])\n",
    "                print(f'{df[feature].min()} - {Vs[feature]} *{w[i]}')\n",
    "                #print(w[i])\n",
    "            else:\n",
    "                delta.append((df[feature].min() - Vs[feature]) * w[i])\n",
    "\n",
    "        # Calcula R como a soma dos deltas menos gamma_A\n",
    "        R = sum(delta) - gamma_A  # Atualiza R para incluir gamma_A\n",
    "        \n",
    "        # Computa a PI-explicação para a instância atual\n",
    "        Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "        # Imprime os resultados\n",
    "        print(f\"\\nInstância {idx}:\")\n",
    "        print(f\"Classe verdadeira: {classe_verdadeira} ({iris.target_names[classe_verdadeira]})\")\n",
    "        print(f\"Probabilidades: {probs}\")\n",
    "        print(f\"Valor de gamma_A: {gamma_A}\")\n",
    "        print(f\"PI-Explicação: \")\n",
    "        for item in Xpl:\n",
    "            print(f\"- {item}\")\n",
    "\n",
    "# Função para calcular a PI-explicação\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    # Ordena o delta junto com seus índices, em ordem decrescente de valor absoluto\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True)  # enumerando os valores de delta em tuplas e ordenando\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]  # Desempacota o índice e o valor do delta\n",
    "        feature = X_test.columns[sorted_idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação\n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "# Exemplo de uso:\n",
    "# Para analisar uma instância específica, passe o índice dela, como 0, 1, 2, etc.\n",
    "# Exemplo: analisar_instancias(0)  -> analisa a instância 0\n",
    "# Para analisar todas as instâncias, apenas execute a função sem argumento\n",
    "analisar_instancias()  # Analisa todas as instâncias\n",
    "# Ou para uma específica\n",
    "# analisar_instancias(2)  # Analisa a instância 2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CODIGO PARTICIONADO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.1\n",
      "- sepal width (cm) - 2.8\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.7\n",
      "- sepal width (cm) - 3.8\n",
      "PI-Explicação: \n",
      "Nenhuma PI-explicação encontrada para esta instância.\n",
      "\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.0\n",
      "- sepal width (cm) - 2.9\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.8\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.4\n",
      "- sepal width (cm) - 3.4\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.6\n",
      "- sepal width (cm) - 2.9\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.9\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.2\n",
      "- sepal width (cm) - 2.2\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.8\n",
      "- sepal width (cm) - 2.7\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.5\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 4.8\n",
      "- sepal width (cm) - 3.0\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.5\n",
      "- sepal width (cm) - 3.5\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 4.9\n",
      "- sepal width (cm) - 3.1\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.1\n",
      "- sepal width (cm) - 3.8\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.3\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.5\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.6\n",
      "- sepal width (cm) - 2.5\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.7\n",
      "- sepal width (cm) - 2.8\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.4\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 4.7\n",
      "- sepal width (cm) - 3.2\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.1\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 5.0\n",
      "- sepal width (cm) - 3.4\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.4\n",
      "PI-Explicação: \n",
      "Nenhuma PI-explicação encontrada para esta instância.\n",
      "\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.7\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.7\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.8\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 4.8\n",
      "- sepal width (cm) - 3.0\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 4.8\n",
      "- sepal width (cm) - 3.1\n"
     ]
    }
   ],
   "source": [
    "#Usando este código\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)  # organizando em um DataFrame\n",
    "df['target'] = iris.target  # rótulos\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treina o modelo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train)  # treinando o modelo\n",
    "\n",
    "# Função para analisar instâncias\n",
    "def analisar_instancias(instancia_para_analisar=1):\n",
    "    num_instancias = len(X_test)\n",
    "    \n",
    "    # Se nenhuma instância for especificada, analisa todas as instâncias\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "    \n",
    "    # Loop para analisar instâncias\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X_test.iloc[idx].to_dict()  # Utilizando a instância de teste especificada\n",
    "        instancia_test = X_test.iloc[[idx]]  # Mantém como DataFrame para preservar os nomes das características\n",
    "\n",
    "        # Previsão de probabilidades para a instância\n",
    "        probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "        # O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "        classe_verdadeira = y_test.iloc[idx]  # Obtém a classe verdadeira da instância\n",
    "        gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "        \n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]  # pesos do modelo treinado\n",
    "\n",
    "        # Calcula o delta para cada feature\n",
    "        for i, feature in enumerate(df.columns[:-1]):\n",
    "            if w[i] < 0:\n",
    "                delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "            else:\n",
    "                delta.append((df[feature].min() - Vs[feature]) * w[i])\n",
    "\n",
    "        # Calcula R como a soma dos deltas menos gamma_A\n",
    "        R = sum(delta) - gamma_A  # Atualiza R para incluir gamma_A\n",
    "        # Computa a PI-explicação para a instância atual\n",
    "        Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "        # Imprime os resultados\n",
    "    #    print(f\"\\nInstância {idx}:\")\n",
    "    #    print(f\"Classe verdadeira: {classe_verdadeira} ({iris.target_names[classe_verdadeira]})\")\n",
    "    #    print(f\"Probabilidades: {probs}\")\n",
    "    #    print(f\"Valor de gamma_A: {gamma_A}\")\n",
    "        print(f\"PI-Explicação: \")\n",
    "        for item in Xpl:\n",
    "            print(f\"- {item}\")\n",
    "        if not Xpl:\n",
    "            print(\"Nenhuma PI-explicação encontrada para esta instância.\\n\")\n",
    "# Função para calcular a PI-explicação\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    \n",
    "    # Ordena o delta em ordem decrescente de valor absoluto\n",
    "    delta_sorted = sorted(delta, reverse=True)  # enumerando os valores de delta em tuplas e ordenando\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        delta_value = delta_sorted[Idx]  # Desempacota o índice e o valor do delta\n",
    "        feature = X_test.columns[Idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação\n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "analisar_instancias(None)  # Analisa todas as instâncias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.8277146135745481\n",
      "Probabilidades: [0.00380009 0.82771461 0.16848529]\n",
      "Classe verdadeira: 1\n",
      "Valor de gamma_A: 0.8277146135745481\n",
      "PI-Explicação: \n",
      "- sepal length (cm) - 6.1\n",
      "- sepal width (cm) - 2.8\n",
      "- petal length (cm) - 4.7\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)  # organizando em um DataFrame\n",
    "df['target'] = iris.target  # rótulos\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treina o modelo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train)  # treinando o modelo\n",
    "\n",
    "# Usa a primeira instância dos dados de teste para a explicação\n",
    "Vs = X_test.iloc[0].to_dict()  # utilizando a primeira instância dos dados de teste \n",
    "instancia_test = X_test.iloc[[0]]  # Mantem como DataFrame para preservar os nomes das características\n",
    "\n",
    "\n",
    "\n",
    "# Previsão de probabilidades para a instância\n",
    "# o predict_proba calcula a probabilidade de uma instância pertencer a uma das classes possíveis usando um modelo de classificação treinado.\n",
    "probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "# O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "classe_verdadeira = y_test.iloc[0]  # Obtém a classe verdadeira da instância\n",
    "gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "print(classe_verdadeira)\n",
    "print(gamma_A)\n",
    "# Cálculo do valor delta para cada feature\n",
    "delta = []\n",
    "w = modelo.coef_[0]  # pesos do modelo treinado\n",
    "\n",
    "# Calcula o delta para cada feature\n",
    "for i, feature in enumerate(df.columns[:-1]):\n",
    "    if w[i] < 0:\n",
    "        delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "    else:\n",
    "        delta.append((Vs[feature] - df[feature].min()) * w[i])\n",
    "\n",
    "# Calcula R como a soma dos deltas menos gamma_A\n",
    "R = sum(delta) - gamma_A  # Atualiza R para incluir gamma_A\n",
    "\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    \n",
    "    # Ordena o delta em ordem decrescente de valor absoluto\n",
    "    delta_sorted = sorted(delta, reverse=True)  # enumerando os valores de delta em tuplas e ordenando\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        delta_value = delta_sorted[Idx]  # Desempacota o índice e o valor do delta\n",
    "        feature = X_test.columns[Idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação\n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "# Computa a PI-explicação\n",
    "Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "# Imprime os resultados\n",
    "print(f\"Probabilidades: {probs}\")\n",
    "print(f\"Classe verdadeira: {classe_verdadeira}\")\n",
    "print(f\"Valor de gamma_A: {gamma_A}\")\n",
    "print(f\"PI-Explicação: \")\n",
    "for item in Xpl:\n",
    "    print(f\"- {item}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Delta -> sepal length (cm) -> 0.07876280643270722\n",
      "Vs -> 7.7\n",
      " df -> 7.9-peso> -0.3938140321635358\n",
      "Delta -> sepal width (cm) -> -0.5773233879333367\n",
      "Vs -> 2.6\n",
      " df -> 4.4-peso> 0.962205646555561\n",
      "Delta -> petal length (cm) -> -0.0\n",
      "Vs -> 6.9\n",
      " df -> 6.9-peso> -2.375192750974152\n",
      "Delta -> petal width (cm) -> 0.1997454612360671\n",
      "Vs -> 2.3\n",
      " df -> 2.5-peso> -0.9987273061803346\n",
      "Valor de R para a instância 2: -1.2972663601677654\n",
      "[0.07876280643270722, -0.5773233879333367, -0.0, 0.1997454612360671]\n",
      "valores -> soma dos delta + gamma_A -> 0.29881512026456236: 0.9984512399032031\n",
      "\n",
      "Instância 2:\n",
      "Classe verdadeira: 2 (virginica)\n",
      "Probabilidades: [8.84412614e-09 1.54875125e-03 9.98451240e-01]\n",
      "Valor de gamma_A: 0.9984512399032031\n",
      "PI-Explicação: \n",
      "Nenhuma PI-explicação encontrada para esta instância.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)  # Organizando em um DataFrame\n",
    "df['target'] = iris.target  # Rótulos\n",
    "\n",
    "# Divide os dados em conjunto de treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Treina o modelo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X_train, y_train)  # Treinando o modelo\n",
    "\n",
    "# Função para analisar instâncias\n",
    "def analisar_instancias(instancia_para_analisar=None):\n",
    "    num_instancias = len(X_test)\n",
    "    \n",
    "    # Se nenhuma instância for especificada, analisa todas as instâncias\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "    \n",
    "    # Loop para analisar instâncias\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X_test.iloc[idx].to_dict()  # Utilizando a instância de teste especificada\n",
    "        instancia_test = X_test.iloc[[idx]]  # Mantém como DataFrame para preservar os nomes das características\n",
    "\n",
    "        # Previsão de probabilidades para a instância\n",
    "        probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "        # O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "        classe_verdadeira = y_test.iloc[idx]  # Obtém a classe verdadeira da instância\n",
    "        gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "        \n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]  # Pesos do modelo treinado\n",
    "\n",
    "        # Calcula o delta para cada feature\n",
    "        for i, feature in enumerate(df.columns[:-1]):\n",
    "            if w[i] < 0:\n",
    "                delta_val = (Vs[feature] - df[feature].max()) * w[i]\n",
    "            else:\n",
    "                delta_val = (df[feature].min() - Vs[feature]) * w[i]\n",
    "                \n",
    "            delta.append(delta_val)\n",
    "            print(f\"Delta -> {feature} -> {delta_val}\")\n",
    "            print(f'Vs -> {Vs[feature]}')\n",
    "            print(f' df -> {df[feature].max()}-peso> {w[i]}')\n",
    "        # Calcula R como a soma dos deltas menos gamma_A\n",
    "        R = sum(delta) - gamma_A\n",
    "        print(f\"Valor de R para a instância {idx}: {R}\")\n",
    "        print(delta)\n",
    "        print(f\"valores -> soma dos delta + gamma_A -> {abs(sum(delta))}: {gamma_A}\")\n",
    "        # Computa a PI-explicação para a instância atual\n",
    "        Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "        # Imprime os resultados\n",
    "        print(f\"\\nInstância {idx}:\")\n",
    "        print(f\"Classe verdadeira: {classe_verdadeira} ({iris.target_names[classe_verdadeira]})\")\n",
    "        print(f\"Probabilidades: {probs}\")\n",
    "        print(f\"Valor de gamma_A: {gamma_A}\")\n",
    "        print(f\"PI-Explicação: \")\n",
    "        for item in Xpl:\n",
    "            print(f\"- {item}\")\n",
    "        sem_explicacao = []\n",
    "        if not Xpl:\n",
    "            sem_explicacao.append(idx)\n",
    "            print(\"Nenhuma PI-explicação encontrada para esta instância.\\n\")\n",
    "        \n",
    "# Função para calcular a PI-explicação\n",
    "def one_explanation(Vs, delta, R):\n",
    "    Xpl = []  # Inicializa a lista de PI-explicação\n",
    "    # Ordena o delta junto com seus índices, em ordem decrescente de valor absoluto\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: x[1], reverse=True)\n",
    "    R_atual = R  # Inicializa o limiar atual\n",
    "    Idx = 0  # Inicializa o índice para iterar\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]  # Desempacota o índice e o valor do delta\n",
    "        feature = X_test.columns[sorted_idx]  # Obtém o nome da feature correspondente\n",
    "        feature_value = Vs[feature]  # Obtém o valor da feature na instância Vs\n",
    "        \n",
    "        # Adiciona à explicação\n",
    "        Xpl.append(f\"{feature} - {feature_value}\")  # Adiciona feature e valor à explicação\n",
    "        \n",
    "        R_atual -= delta_value  # Atualiza o limiar atual para manter ou parar o loop\n",
    "        Idx += 1  # Incrementa o índice\n",
    "    \n",
    "    return Xpl  # Retorna a PI-explicação\n",
    "\n",
    "# Executa a análise na instância 2 para verificar o problema\n",
    "analisar_instancias(2)  # Analisa a instância 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valores da Instância 2:\n",
      "sepal length (cm)    7.9\n",
      "sepal width (cm)     3.8\n",
      "petal length (cm)    6.4\n",
      "petal width (cm)     2.0\n",
      "Name: 131, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Exibe os valores das características da instância 2\n",
    "print(\"Valores da Instância 2:\")\n",
    "print(X_test.iloc[24])  # Acessa a linha 2 (índice 2) em X_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Instâncias sem PI-explicação: [2, 24]\n"
     ]
    }
   ],
   "source": [
    "# Lista para armazenar os índices das instâncias sem PI-explicação\n",
    "instancias_sem_explicacao = []\n",
    "\n",
    "# Itera sobre cada instância dos dados de teste\n",
    "for idx in range(len(X_test)):\n",
    "    Vs = X_test.iloc[[idx]].to_dict('records')[0]  # Obtendo cada instancia como um dicionario\n",
    "    instancia_test = X_test.iloc[[idx]]  # Mantem como DataFrame para preservar os nomes das características\n",
    "\n",
    "    # Previsão de probabilidades para a instância\n",
    "    probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "    # O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "    classe_verdadeira = y_test.iloc[idx]  # Obtém a classe verdadeira da instância\n",
    "    gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "\n",
    "    # Calcula o delta para cada feature\n",
    "    delta = []\n",
    "    for i, feature in enumerate(df.columns[:-1]):\n",
    "        if w[i] < 0:\n",
    "            delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "        else:\n",
    "            delta.append((df[feature].min() - Vs[feature]) * w[i])\n",
    "\n",
    "    # Calcula R como a soma dos deltas menos gamma_A\n",
    "    R = sum(delta) - gamma_A  # Atualiza R para incluir gamma_A\n",
    "\n",
    "    # Computa a PI-explicação\n",
    "    Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "    # Verifica se a PI-explicação está vazia\n",
    "    if not Xpl:\n",
    "        instancias_sem_explicacao.append(idx)  # Adiciona o índice da instância à lista\n",
    "\n",
    "# Imprime a lista de instâncias sem PI-explicação\n",
    "print(f\"Instâncias sem PI-explicação: {instancias_sem_explicacao}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Instância 2:\n",
      "Classe: 0 -> setosa\n",
      "PI-Explicação: \n",
      "- petal length (cm) - 1.3\n",
      "- petal width (cm) - 0.2\n",
      "- sepal length (cm) - 4.7\n",
      "- sepal width (cm) - 3.2\n"
     ]
    }
   ],
   "source": [
    "# Carrega o dataset Iris completo\n",
    "X = df.drop('target', axis=1)  # Utilizando todas as características\n",
    "y = df['target']  # Utilizando todos os rótulos\n",
    "\n",
    "# Treina o modelo usando o dataset completo\n",
    "modelo = LogisticRegression(max_iter=200)\n",
    "modelo.fit(X, y)  # Treinando o modelo com todas as instâncias\n",
    "\n",
    "# Modifique a função `analisar_instancias` para utilizar o dataset completo\n",
    "def analisar_instancias(instancia_para_analisar=None):\n",
    "    num_instancias = len(X)\n",
    "\n",
    "    # Se nenhuma instância for especificada, analisa todas as instâncias\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "\n",
    "    # Loop para analisar instâncias\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X.iloc[idx].to_dict()  # Utilizando a instância especificada\n",
    "        instancia_test = X.iloc[[idx]]  # Mantém como DataFrame para preservar os nomes das características\n",
    "\n",
    "        # Previsão de probabilidades para a instância\n",
    "        probs = modelo.predict_proba(instancia_test)[0]  # Obtém as probabilidades para cada classe\n",
    "\n",
    "        # O valor de gamma_A é a probabilidade da classe verdadeira\n",
    "        classe_verdadeira = y.iloc[idx]  # Obtém a classe verdadeira da instância\n",
    "        gamma_A = probs[classe_verdadeira]  # Extrai a probabilidade correspondente à classe verdadeira\n",
    "\n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]  # Pesos do modelo treinado\n",
    "\n",
    "        # Calcula o delta para cada feature\n",
    "        for i, feature in enumerate(df.columns[:-1]):\n",
    "            if w[i] < 0:\n",
    "                delta.append((Vs[feature] - df[feature].max()) * w[i])\n",
    "            else:\n",
    "                delta.append((Vs[feature] - df[feature].min()) * w[i])\n",
    "\n",
    "        # Calcula R como a soma dos deltas menos gamma_A\n",
    "        R = sum(delta) - gamma_A\n",
    "        Xpl = one_explanation(Vs, delta, R)\n",
    "\n",
    "        # Imprime os resultados\n",
    "        print(f\"\\nInstância {idx}:\")\n",
    "        print(f\"Classe: {classe_verdadeira} -> {iris.target_names[classe_verdadeira]}\")\n",
    "        print(f\"PI-Explicação: \")\n",
    "        for item in Xpl:\n",
    "            print(f\"- {item}\")\n",
    "        if not Xpl:\n",
    "            print('_No-PI-explanation_'*3)\n",
    "\n",
    "# Chamada da função para analisar todas as instâncias\n",
    "analisar_instancias(2)  # Analisa todas as 150 instâncias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
