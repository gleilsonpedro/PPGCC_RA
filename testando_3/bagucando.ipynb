{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'diabetes' escolhido.\n",
      "Dataset diabetes carregado com sucesso.\n",
      "Classes: [151.  75. 141. 206. 135.  97. 138.  63. 110. 310. 101.  69. 179. 185.\n",
      " 118. 171. 166. 144.  97. 168.  68.  49.  68. 245. 184. 202. 137.  85.\n",
      " 131. 283. 129.  59. 341.  87.  65. 102. 265. 276. 252.  90. 100.  55.\n",
      "  61.  92. 259.  53. 190. 142.  75. 142. 155. 225.  59. 104. 182. 128.\n",
      "  52.  37. 170. 170.  61. 144.  52. 128.  71. 163. 150.  97. 160. 178.\n",
      "  48. 270. 202. 111.  85.  42. 170. 200. 252. 113. 143.  51.  52. 210.\n",
      "  65. 141.  55. 134.  42. 111.  98. 164.  48.  96.  90. 162. 150. 279.\n",
      "  92.  83. 128. 102. 302. 198.  95.  53. 134. 144. 232.  81. 104.  59.\n",
      " 246. 297. 258. 229. 275. 281. 179. 200. 200. 173. 180.  84. 121. 161.\n",
      "  99. 109. 115. 268. 274. 158. 107.  83. 103. 272.  85. 280. 336. 281.\n",
      " 118. 317. 235.  60. 174. 259. 178. 128.  96. 126. 288.  88. 292.  71.\n",
      " 197. 186.  25.  84.  96. 195.  53. 217. 172. 131. 214.  59.  70. 220.\n",
      " 268. 152.  47.  74. 295. 101. 151. 127. 237. 225.  81. 151. 107.  64.\n",
      " 138. 185. 265. 101. 137. 143. 141.  79. 292. 178.  91. 116.  86. 122.\n",
      "  72. 129. 142.  90. 158.  39. 196. 222. 277.  99. 196. 202. 155.  77.\n",
      " 191.  70.  73.  49.  65. 263. 248. 296. 214. 185.  78.  93. 252. 150.\n",
      "  77. 208.  77. 108. 160.  53. 220. 154. 259.  90. 246. 124.  67.  72.\n",
      " 257. 262. 275. 177.  71.  47. 187. 125.  78.  51. 258. 215. 303. 243.\n",
      "  91. 150. 310. 153. 346.  63.  89.  50.  39. 103. 308. 116. 145.  74.\n",
      "  45. 115. 264.  87. 202. 127. 182. 241.  66.  94. 283.  64. 102. 200.\n",
      " 265.  94. 230. 181. 156. 233.  60. 219.  80.  68. 332. 248.  84. 200.\n",
      "  55.  85.  89.  31. 129.  83. 275.  65. 198. 236. 253. 124.  44. 172.\n",
      " 114. 142. 109. 180. 144. 163. 147.  97. 220. 190. 109. 191. 122. 230.\n",
      " 242. 248. 249. 192. 131. 237.  78. 135. 244. 199. 270. 164.  72.  96.\n",
      " 306.  91. 214.  95. 216. 263. 178. 113. 200. 139. 139.  88. 148.  88.\n",
      " 243.  71.  77. 109. 272.  60.  54. 221.  90. 311. 281. 182. 321.  58.\n",
      " 262. 206. 233. 242. 123. 167.  63. 197.  71. 168. 140. 217. 121. 235.\n",
      " 245.  40.  52. 104. 132.  88.  69. 219.  72. 201. 110.  51. 277.  63.\n",
      " 118.  69. 273. 258.  43. 198. 242. 232. 175.  93. 168. 275. 293. 281.\n",
      "  72. 140. 189. 181. 209. 136. 261. 113. 131. 174. 257.  55.  84.  42.\n",
      " 146. 212. 233.  91. 111. 152. 120.  67. 310.  94. 183.  66. 173.  72.\n",
      "  49.  64.  48. 178. 104. 132. 220.  57.]\n",
      "Amostras: 442 | Atributos: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits, load_diabetes, fetch_california_housing, fetch_openml\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "nome_dataset = []\n",
    "# Função para carregar os datasets\n",
    "def carregar_dataset(nome_dataset):\n",
    "    if nome_dataset == 'iris':\n",
    "        data = load_iris()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'wine':\n",
    "        data = load_wine()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'digits':\n",
    "        data = load_digits()\n",
    "        X, y = pd.DataFrame(data.data, columns=[f\"pixel_{i}\" for i in range(data.data.shape[1])]), data.target\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "    \n",
    "    elif nome_dataset == 'diabetes':\n",
    "        data = load_diabetes()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target  # Dados de regressão, podem ser classificados em faixas\n",
    "    \n",
    "    elif nome_dataset == 'boston_housing':\n",
    "        data = fetch_openml(name='boston', version=1)\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = ['Price']  # Regressão, mas pode ser categorizada\n",
    "    \n",
    "    elif nome_dataset == 'california_housing':\n",
    "        data = fetch_california_housing()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = ['Price']\n",
    "    \n",
    "    elif nome_dataset == 'wine_quality':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "        X = data.drop(columns=['quality'])\n",
    "        y = data['quality']\n",
    "        class_names = sorted(y.unique().tolist())\n",
    "    \n",
    "    elif nome_dataset == 'heart_disease':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header=None, na_values=\"?\")\n",
    "        data = data.dropna()  # Remove missing values\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "        class_names = sorted(y.unique().tolist())\n",
    "    \n",
    "    elif nome_dataset == 'banknote_authentication':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", header=None)\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "        class_names = ['Legitimate', 'Forgery']\n",
    "    \n",
    "    elif nome_dataset == 'parkinsons':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n",
    "        X = data.drop(columns=['status', 'name'])\n",
    "        y = data['status']\n",
    "        class_names = ['Healthy', 'Parkinsons']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Nome do dataset não reconhecido. Escolha um dataset válido.\")\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "\n",
    "\n",
    "mostrar = {'menu' : '\\n|  ************************* MENU ***************************  |\\\n",
    "        \\n|  0 - iris                     |  1 - wine                     |\\\n",
    "        \\n|  2 - breats_cancer            |  3 - digits                  |\\\n",
    "        \\n|  4 - diabetes                 |  5 - boston_housing          |\\\n",
    "        \\n|  6 - california_housing       |  7 - wine_quality            |\\\n",
    "        \\n|  8 - heart_disease            |  9 - banknote_authentication|\\\n",
    "        \\n|  10 - parkingsons             |  Q - SAIR                    |\\\n",
    "        \\n|-------------------------------------------------------------|'}\n",
    "\n",
    "print(mostrar['menu'])\n",
    "opcao = input(\"Digite o número do dataset ou 'Q' para sair: \").upper().strip()\n",
    "    # Processa a opção\n",
    "while True:\n",
    "    if opcao == 'Q':\n",
    "        clear_output()\n",
    "        print(\"Você escolheu sair.\")\n",
    "        break\n",
    "    elif opcao.isdigit() and 0 <= int(opcao) <= 10:\n",
    "        nomes_datasets = ['iris', 'wine', 'breast_cancer', 'digits', 'diabetes','boston_housing',   'california_housing', 'wine_quality', 'heart_disease','banknote_authentication',  'parkinsons']\n",
    "        nome_dataset = nomes_datasets[int(opcao)]\n",
    "        clear_output()\n",
    "        print(f\"Dataset '{nome_dataset}' escolhido.\")\n",
    "        try:\n",
    "            X, y, class_names = carregar_dataset(nome_dataset)\n",
    "            print(f\"Dataset {nome_dataset} carregado com sucesso.\")\n",
    "            print(\"Classes:\", class_names)\n",
    "            print(\"Amostras:\", X.shape[0], \"| Atributos:\", X.shape[1])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o dataset: {e}\")\n",
    "    else:\n",
    "        clear_output()\n",
    "        print(\"Siga as instruções do menu ou digite 'S' para sair.\")\n",
    "        break\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[58], line 130\u001b[0m\n\u001b[0;32m    127\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m min_val, max_val\n\u001b[0;32m    129\u001b[0m \u001b[38;5;66;03m# Exemplo de uso (substitua X e y pelos dados adequados)\u001b[39;00m\n\u001b[1;32m--> 130\u001b[0m \u001b[43manalisar_instancias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclasse_0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    131\u001b[0m \u001b[38;5;66;03m# Crie um dicionário para armazenar a contagem de cada feature\u001b[39;00m\n\u001b[0;32m    132\u001b[0m contagem_features \u001b[38;5;241m=\u001b[39m {}\n",
      "Cell \u001b[1;32mIn[58], line 22\u001b[0m, in \u001b[0;36manalisar_instancias\u001b[1;34m(X, y, class_names, classe_0, instancia_para_analisar)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;66;03m# Treina o modelo\u001b[39;00m\n\u001b[0;32m     21\u001b[0m modelo \u001b[38;5;241m=\u001b[39m LogisticRegression(max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m200\u001b[39m)\n\u001b[1;32m---> 22\u001b[0m \u001b[43mmodelo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# Obtém os nomes das features\u001b[39;00m\n\u001b[0;32m     25\u001b[0m feature_names \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mtolist()  \n",
      "File \u001b[1;32mc:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:1246\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m   1244\u001b[0m classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_\n\u001b[0;32m   1245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_classes \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m-> 1246\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m   1247\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis solver needs samples of at least 2 classes\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1248\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m in the data, but the data contains only one\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1249\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m class: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1250\u001b[0m         \u001b[38;5;241m%\u001b[39m classes_[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m   1251\u001b[0m     )\n\u001b[0;32m   1253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses_) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m   1254\u001b[0m     n_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: This solver needs samples of at least 2 classes in the data, but the data contains only one class: 0"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Função para transformar o dataset em um problema binário (classe 0 contra as outras)\n",
    "def transformar_problema_binario(y, classe_0):\n",
    "    return [1 if label == classe_0 else 0 for label in y]\n",
    "\n",
    "def analisar_instancias(X, y, class_names, classe_0=0, instancia_para_analisar=None):\n",
    "    global TUDO\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    # Transforma o problema em binário\n",
    "    y_binario = transformar_problema_binario(y, classe_0)\n",
    "\n",
    "    # Divide o dataset em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_binario, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Treina o modelo\n",
    "    modelo = LogisticRegression(max_iter=200)\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Obtém os nomes das features\n",
    "    feature_names = X.columns.tolist()  \n",
    "    \n",
    "    # Seleciona as instâncias para análise\n",
    "    num_instancias = len(X_test)\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "    \n",
    "    TUDO = []\n",
    "    # Loop para analisar cada instância selecionada\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X_test.iloc[idx].to_dict()\n",
    "        instancia_test = X_test.iloc[[idx]]\n",
    "\n",
    "        # Calcula `gamma_A` usando `decision_function`\n",
    "        gamma_A = modelo.decision_function(instancia_test)[0]\n",
    "        \n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            if w[i] < 0:\n",
    "                delta.append((Vs[feature] - X[feature].max()) * w[i])\n",
    "            else:\n",
    "                delta.append((Vs[feature] - X[feature].min()) * w[i])\n",
    "\n",
    "        # Calcula R\n",
    "        R = sum(delta) - gamma_A\n",
    "        \n",
    "        # Computa a PI-explicação para a instância atual usando nomes das features\n",
    "        Xpl = one_explanation(Vs, delta, R, feature_names, modelo, instancia_test, X)\n",
    "        # Imprime os resultados\n",
    "        classe_verdadeira = y_test[idx]\n",
    "        print(f\"\\nInstância {idx}:\")\n",
    "        print(f\"Classe verdadeira (binária): {classe_verdadeira}\")\n",
    "        print(f\"PI-Explicação: \")\n",
    "        \n",
    "        TUDO.append(Xpl)\n",
    "\n",
    "        for item in Xpl:\n",
    "            print(f\"- {item}\")\n",
    "            \n",
    "            \n",
    "        if not Xpl:\n",
    "            print('_No-PI-explanation_'*3)\n",
    "        \n",
    "\n",
    "# Função para calcular a PI-explicação e incluir os intervalos de valores mínimos e máximos que garantem a classe\n",
    "def one_explanation(Vs, delta, R, feature_names, modelo, instancia_test, X):\n",
    "    Xpl = []\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True)\n",
    "    R_atual = R\n",
    "    Idx = 0\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]\n",
    "        feature = feature_names[sorted_idx]\n",
    "        feature_value = Vs[feature]\n",
    "\n",
    "        # Encontra os valores mínimo e máximo para manter a classe usando perturbação\n",
    "        #min_val, max_val = encontrar_intervalo_perturbacao(modelo, instancia_test, feature, feature_value, classe_desejada=1, X=X)\n",
    "\n",
    "        # Adiciona a feature com o valor da instância e o intervalo mínimo/máximo que mantém a classe\n",
    "        Xpl.append(f\"{feature} - {feature_value} \")\n",
    "        R_atual -= delta_value\n",
    "        Idx += 1\n",
    "    \n",
    "    return Xpl\n",
    "\n",
    "# Função para encontrar o intervalo de perturbação para manter a classe, considerando limites de X\n",
    "def encontrar_intervalo_perturbacao(modelo, instancia, feature, valor_original, classe_desejada, X, passo=0.1, max_iter=50):\n",
    "    # Define os valores mínimo e máximo baseados nos dados de entrada\n",
    "    min_val_data = X[feature].min()\n",
    "    max_val_data = X[feature].max()\n",
    "    \n",
    "    # Inicializa os valores mínimo e máximo com o valor da instância\n",
    "    min_val, max_val = valor_original, valor_original\n",
    "    \n",
    "    # Perturba negativamente\n",
    "    for _ in range(max_iter):\n",
    "        min_val -= passo\n",
    "        if min_val < min_val_data:\n",
    "            min_val = min_val_data\n",
    "            break\n",
    "        instancia_perturbada = instancia.copy()\n",
    "        instancia_perturbada[feature] = min_val\n",
    "        predicao = modelo.predict(instancia_perturbada)\n",
    "        if predicao[0] != classe_desejada:\n",
    "            min_val += passo\n",
    "            break\n",
    "\n",
    "    # Perturba positivamente\n",
    "    for _ in range(max_iter):\n",
    "        max_val += passo\n",
    "        if max_val > max_val_data:\n",
    "            max_val = max_val_data\n",
    "            break\n",
    "        instancia_perturbada = instancia.copy()\n",
    "        instancia_perturbada[feature] = max_val\n",
    "        predicao = modelo.predict(instancia_perturbada)\n",
    "        if predicao[0] != classe_desejada:\n",
    "            max_val -= passo\n",
    "            break\n",
    "\n",
    "    return min_val, max_val\n",
    "\n",
    "# Exemplo de uso (substitua X e y pelos dados adequados)\n",
    "analisar_instancias(X, y, class_names, classe_0=0)\n",
    "# Crie um dicionário para armazenar a contagem de cada feature\n",
    "contagem_features = {}\n",
    "\n",
    "# Itere sobre cada item da lista TUDO\n",
    "for item in TUDO:\n",
    "    # Verifique se o item é uma lista\n",
    "    if isinstance(item, list):\n",
    "        # Itere sobre cada item da lista\n",
    "        for feature in item:\n",
    "            # Extraia o nome da feature\n",
    "            nome_feature = feature.split(\" - \")[0]\n",
    "\n",
    "            # Verifique se a feature já está no dicionário\n",
    "            if nome_feature in contagem_features:\n",
    "                # Incremente a contagem\n",
    "                contagem_features[nome_feature] += 1\n",
    "            else:\n",
    "                # Adicione a feature ao dicionário com contagem 1\n",
    "                contagem_features[nome_feature] = 1\n",
    "\n",
    "# Imprima as features e suas contagens\n",
    "for nome_feature, contagem in contagem_features.items():\n",
    "    print(f\"Feature: {nome_feature} Contagem: {contagem}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
