{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET DE REGRESSÃO\n",
    "from sklearn.datasets import fetch_california_housing, load_diabetes, make_regression, load_linnerud, fetch_openml\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "import numpy as np\n",
    "\n",
    "# Função para carregar e transformar os datasets de regressão em problemas binários\n",
    "def carregar_dataset_regressao_binario(nome_dataset):\n",
    "    if nome_dataset == 'california_housing_binary':\n",
    "        data = fetch_california_housing()\n",
    "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        y = pd.qcut(data.target, q=2, labels=[0, 1]).astype(int)  # Dividindo pela mediana\n",
    "        class_names = ['Low Price', 'High Price']\n",
    "\n",
    "    elif nome_dataset == 'ames_housing_binary':\n",
    "        data = fetch_openml(name='house_prices', as_frame=True)\n",
    "        X = data.data.select_dtypes(include=[np.number])  # Seleciona apenas colunas numéricas\n",
    "        y = pd.qcut(data.target, q=2, labels=[0, 1]).astype(int)  # Binarizando pela mediana\n",
    "        class_names = ['Low Price', 'High Price']\n",
    "\n",
    "    elif nome_dataset == 'diabetes_binary':\n",
    "        data = load_diabetes()\n",
    "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        y = pd.qcut(data.target, q=2, labels=[0, 1]).astype(int)  # Transformando pela mediana\n",
    "        class_names = ['Low Progression', 'High Progression']\n",
    "\n",
    "    elif nome_dataset == 'synthetic_regression_binary':\n",
    "        X, y_cont = make_regression(n_samples=500, n_features=10, noise=0.1)\n",
    "        y = pd.qcut(y_cont, q=2, labels=[0, 1]).astype(int)  # Dividindo os valores de y pela mediana\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        class_names = ['Low Output', 'High Output']\n",
    "\n",
    "    elif nome_dataset == 'linnerud_binary':\n",
    "        data = load_linnerud()\n",
    "        X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "        y = pd.qcut(data.target[:, 0], q=2, labels=[0, 1]).astype(int)  # Convertendo pela primeira coluna\n",
    "        class_names = ['Low Fitness', 'High Fitness']\n",
    "\n",
    "    elif nome_dataset == 'carbon_nanotubes_binary':\n",
    "        # Gerando um dataset fixo para evitar problemas de aleatoriedade\n",
    "        np.random.seed(42)\n",
    "        X = pd.DataFrame(np.random.rand(500, 6), columns=[f\"feature_{i}\" for i in range(6)])\n",
    "        y = np.where(X['feature_0'] > X['feature_0'].median(), 1, 0)  # Binarização com base em uma coluna\n",
    "        class_names = ['Low Value', 'High Value']\n",
    "\n",
    "    elif nome_dataset == 'random_dataset_1':\n",
    "        X, y_cont = make_regression(n_samples=500, n_features=8, noise=0.5, random_state=1)\n",
    "        y = pd.qcut(y_cont, q=2, labels=[0, 1]).astype(int)\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        class_names = ['Low Outcome', 'High Outcome']\n",
    "\n",
    "    elif nome_dataset == 'random_dataset_2':\n",
    "        X, y_cont = make_regression(n_samples=500, n_features=5, noise=1.0, random_state=2)\n",
    "        y = pd.qcut(y_cont, q=2, labels=[0, 1]).astype(int)\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        class_names = ['Low Output', 'High Output']\n",
    "\n",
    "    elif nome_dataset == 'random_dataset_3':\n",
    "        X, y_cont = make_regression(n_samples=500, n_features=7, noise=0.3, random_state=3)\n",
    "        y = pd.qcut(y_cont, q=2, labels=[0, 1]).astype(int)\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        class_names = ['Low Value', 'High Value']\n",
    "\n",
    "    elif nome_dataset == 'random_dataset_4':\n",
    "        X, y_cont = make_regression(n_samples=500, n_features=6, noise=0.7, random_state=4)\n",
    "        y = pd.qcut(y_cont, q=2, labels=[0, 1]).astype(int)\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "        class_names = ['Low Result', 'High Result']\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Nome do dataset não reconhecido. Escolha um dataset válido.\")\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "# Menu de seleção de datasets de regressão\n",
    "menu = '''\n",
    "|  ************************* MENU ***************************  |\n",
    "|  0 - california_housing_binary   |  1 - ames_housing_binary    |\n",
    "|  2 - diabetes_binary             |  3 - synthetic_regression_binary |\n",
    "|  4 - linnerud_binary             |  5 - carbon_nanotubes_binary |\n",
    "|  6 - random_dataset_1            |  7 - random_dataset_2        |\n",
    "|  8 - random_dataset_3            |  9 - random_dataset_4        |\n",
    "|  Q - SAIR                                                   |\n",
    "|-------------------------------------------------------------|\n",
    "'''\n",
    "\n",
    "# Loop do menu com validação adequada\n",
    "while True:\n",
    "    print(menu)\n",
    "    opcao = input(\"Digite o número do dataset ou 'Q' para sair: \").upper().strip()\n",
    "\n",
    "    if opcao == 'Q':\n",
    "        clear_output()\n",
    "        print(\"Você escolheu sair.\")\n",
    "        break\n",
    "    elif opcao.isdigit() and 0 <= int(opcao) <= 9:\n",
    "        nomes_datasets = [\n",
    "            'california_housing_binary', 'ames_housing_binary', 'diabetes_binary', 'synthetic_regression_binary',\n",
    "            'linnerud_binary', 'carbon_nanotubes_binary', 'random_dataset_1', 'random_dataset_2',\n",
    "            'random_dataset_3', 'random_dataset_4'\n",
    "        ]\n",
    "        nome_dataset = nomes_datasets[int(opcao)]\n",
    "        clear_output()\n",
    "        print(f\"Dataset '{nome_dataset}' escolhido.\")\n",
    "        try:\n",
    "            X, y, class_names = carregar_dataset_regressao_binario(nome_dataset)\n",
    "            print(f\"Dataset {nome_dataset} carregado com sucesso.\")\n",
    "            print(\"Classes:\", class_names)\n",
    "            print(\"Amostras:\", X.shape[0], \"| Atributos:\", X.shape[1])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o dataset: {e}\")\n",
    "    else:\n",
    "        clear_output()\n",
    "        print(\"Opção inválida. Por favor, escolha um número do menu ou 'Q' para sair.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset 'breast_cancer' escolhido.\n",
      "Dataset breast_cancer carregado com sucesso.\n",
      "Classes: ['malignant' 'benign']\n",
      "Amostras: 569 | Atributos: 30\n"
     ]
    }
   ],
   "source": [
    "# DATASETS DE CLASSIFICAÇÃO\n",
    "from sklearn.datasets import load_iris, load_wine, load_breast_cancer, load_digits\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Função para carregar os datasets\n",
    "def carregar_dataset(nome_dataset):\n",
    "    if nome_dataset == 'iris':\n",
    "        data = load_iris()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'wine':\n",
    "        data = load_wine()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'breast_cancer':\n",
    "        data = load_breast_cancer()\n",
    "        X, y = pd.DataFrame(data.data, columns=data.feature_names), data.target\n",
    "        class_names = data.target_names\n",
    "    \n",
    "    elif nome_dataset == 'digits':\n",
    "        data = load_digits()\n",
    "        X, y = pd.DataFrame(data.data, columns=[f\"pixel_{i}\" for i in range(data.data.shape[1])]), data.target\n",
    "        class_names = [str(i) for i in range(10)]\n",
    "    \n",
    "    elif nome_dataset == 'banknote_authentication':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/00267/data_banknote_authentication.txt\", header=None)\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "        class_names = ['Legitimate', 'Forgery']\n",
    "    \n",
    "    elif nome_dataset == 'wine_quality':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv\", sep=';')\n",
    "        X = data.drop(columns=['quality'])\n",
    "        y = data['quality']\n",
    "        class_names = sorted(y.unique().tolist())\n",
    "    \n",
    "    elif nome_dataset == 'heart_disease':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/heart-disease/processed.cleveland.data\", header=None, na_values=\"?\")\n",
    "        data = data.dropna()  # Remove valores ausentes\n",
    "        X = data.iloc[:, :-1]\n",
    "        y = data.iloc[:, -1]\n",
    "        class_names = sorted(y.unique().tolist())\n",
    "    \n",
    "    elif nome_dataset == 'parkinsons':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/parkinsons/parkinsons.data\")\n",
    "        X = data.drop(columns=['status', 'name'])\n",
    "        y = data['status']\n",
    "        class_names = ['Healthy', 'Parkinsons']\n",
    "    \n",
    "    elif nome_dataset == 'car_evaluation':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/car/car.data\", header=None)\n",
    "        data.columns = ['buying', 'maint', 'doors', 'persons', 'lug_boot', 'safety', 'class']\n",
    "        X = pd.get_dummies(data.drop(columns=['class']))\n",
    "        y = data['class'].factorize()[0]\n",
    "        class_names = data['class'].unique()\n",
    "    \n",
    "    elif nome_dataset == 'diabetes_binary':\n",
    "        data = pd.read_csv(\"https://archive.ics.uci.edu/ml/machine-learning-databases/diabetes/diabetes_data_upload.csv\")\n",
    "        X = pd.get_dummies(data.drop(columns=['class']))\n",
    "        y = data['class'].apply(lambda x: 1 if x == 'Positive' else 0)\n",
    "        class_names = ['Negative', 'Positive']\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(\"Nome do dataset não reconhecido. Escolha um dataset válido.\")\n",
    "    \n",
    "    return X, y, class_names\n",
    "\n",
    "# Menu de seleção de datasets\n",
    "menu = '''\n",
    "|  ************************* MENU ***************************  |\n",
    "|  0 - iris                     |  1 - wine                     |\n",
    "|  2 - breast_cancer            |  3 - digits                  |\n",
    "|  4 - banknote_authentication  |  5 - wine_quality           |\n",
    "|  6 - heart_disease            |  7 - parkinsons             |\n",
    "|  8 - car_evaluation           |  9 - diabetes_binary        |\n",
    "|  Q - SAIR                                                |\n",
    "|-------------------------------------------------------------|\n",
    "'''\n",
    "\n",
    "# Exibe o menu e solicita uma escolha\n",
    "print(menu)\n",
    "opcao = input(\"Digite o número do dataset ou 'Q' para sair: \").upper().strip()\n",
    "\n",
    "# Processa a opção selecionada\n",
    "while True:\n",
    "    if opcao == 'Q':\n",
    "        clear_output()\n",
    "        print(\"Você escolheu sair.\")\n",
    "        break\n",
    "    elif opcao.isdigit() and 0 <= int(opcao) <= 9:\n",
    "        nomes_datasets = [\n",
    "            'iris', 'wine', 'breast_cancer', 'digits', 'banknote_authentication',\n",
    "            'wine_quality', 'heart_disease', 'parkinsons', 'car_evaluation', 'diabetes_binary'\n",
    "        ]\n",
    "        nome_dataset = nomes_datasets[int(opcao)]\n",
    "        clear_output()\n",
    "        print(f\"Dataset '{nome_dataset}' escolhido.\")\n",
    "        try:\n",
    "            X, y, class_names = carregar_dataset(nome_dataset)\n",
    "            print(f\"Dataset {nome_dataset} carregado com sucesso.\")\n",
    "            print(\"Classes:\", class_names)\n",
    "            print(\"Amostras:\", X.shape[0], \"| Atributos:\", X.shape[1])\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"Erro ao processar o dataset: {e}\")\n",
    "    else:\n",
    "        clear_output()\n",
    "        print(menu)\n",
    "        print(\"Opção inválida. Por favor, escolha um número do menu ou 'Q' para sair.\")\n",
    "        opcao = input(\"Digite o número do dataset ou 'Q' para sair: \").upper().strip()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gleilsonpedro\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Função para transformar o dataset em um problema binário (classe 0 contra as outras)\n",
    "def transformar_problema_binario(y, classe_0):\n",
    "    return [1 if label == classe_0 else 0 for label in y]\n",
    "\n",
    "def analisar_instancias(X, y, class_names, classe_0=0, instancia_para_analisar=None):\n",
    "    global TUDO\n",
    "    if not isinstance(X, pd.DataFrame):\n",
    "        X = pd.DataFrame(X, columns=[f\"feature_{i}\" for i in range(X.shape[1])])\n",
    "\n",
    "    # Transforma o problema em binário\n",
    "    y_binario = transformar_problema_binario(y, classe_0)\n",
    "\n",
    "    # Divide o dataset em treino e teste\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y_binario, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Treina o modelo\n",
    "    modelo = LogisticRegression(max_iter=200)\n",
    "    modelo.fit(X_train, y_train)\n",
    "\n",
    "    # Obtém os nomes das features\n",
    "    feature_names = X.columns.tolist()  \n",
    "    \n",
    "    # Seleciona as instâncias para análise\n",
    "    num_instancias = len(X_test)\n",
    "    instancias_para_analisar = range(num_instancias) if instancia_para_analisar is None else [instancia_para_analisar]\n",
    "    \n",
    "    TUDO = []\n",
    "    # Loop para analisar cada instância selecionada\n",
    "    for idx in instancias_para_analisar:\n",
    "        Vs = X_test.iloc[idx].to_dict()\n",
    "        instancia_test = X_test.iloc[[idx]]\n",
    "\n",
    "        # Calcula `gamma_A` usando `decision_function`\n",
    "        gamma_A = modelo.decision_function(instancia_test)[0]\n",
    "        \n",
    "        # Cálculo do valor delta para cada feature\n",
    "        delta = []\n",
    "        w = modelo.coef_[0]\n",
    "        for i, feature in enumerate(feature_names):\n",
    "            if w[i] < 0:\n",
    "                delta.append((Vs[feature] - X[feature].max()) * w[i])\n",
    "            else:\n",
    "                delta.append((Vs[feature] - X[feature].min()) * w[i])\n",
    "\n",
    "        # Calcula R\n",
    "        R = sum(delta) - gamma_A\n",
    "        \n",
    "        # Computa a PI-explicação para a instância atual usando nomes das features\n",
    "        Xpl = one_explanation(Vs, delta, R, feature_names, modelo, instancia_test, X)\n",
    "        # Imprime os resultados\n",
    "        classe_verdadeira = y_test[idx]\n",
    "       # print(f\"\\nInstância {idx}:\")\n",
    "       # print(f\"Classe verdadeira (binária): {classe_verdadeira}\")\n",
    "       # print(f\"PI-Explicação: \")\n",
    "        \n",
    "        TUDO.append(Xpl)\n",
    "\n",
    "       # for item in Xpl:\n",
    "       #     print(f\"- {item}\")\n",
    "            \n",
    "            \n",
    "       # if not Xpl:\n",
    "       #     print('_No-PI-explanation_'*3)\n",
    "        \n",
    "\n",
    "# Função para calcular a PI-explicação e incluir os intervalos de valores mínimos e máximos que garantem a classe\n",
    "def one_explanation(Vs, delta, R, feature_names, modelo, instancia_test, X):\n",
    "    Xpl = []\n",
    "    delta_sorted = sorted(enumerate(delta), key=lambda x: abs(x[1]), reverse=True)\n",
    "    R_atual = R\n",
    "    Idx = 0\n",
    "    \n",
    "    while R_atual >= 0 and Idx < len(delta_sorted):\n",
    "        sorted_idx, delta_value = delta_sorted[Idx]\n",
    "        feature = feature_names[sorted_idx]\n",
    "        feature_value = Vs[feature]\n",
    "\n",
    "        # Encontra os valores mínimo e máximo para manter a classe usando perturbação\n",
    "        #min_val, max_val = encontrar_intervalo_perturbacao(modelo, instancia_test, feature, feature_value, classe_desejada=1, X=X)\n",
    "\n",
    "        # Adiciona a feature com o valor da instância e o intervalo mínimo/máximo que mantém a classe\n",
    "        Xpl.append(f\"{feature} - {feature_value} \")\n",
    "        R_atual -= delta_value\n",
    "        Idx += 1\n",
    "    \n",
    "    return Xpl\n",
    "\n",
    "# Função para encontrar o intervalo de perturbação para manter a classe, considerando limites de X\n",
    "def encontrar_intervalo_perturbacao(modelo, instancia, feature, valor_original, classe_desejada, X, passo=0.1, max_iter=50):\n",
    "    # Define os valores mínimo e máximo baseados nos dados de entrada\n",
    "    min_val_data = X[feature].min()\n",
    "    max_val_data = X[feature].max()\n",
    "    \n",
    "    # Inicializa os valores mínimo e máximo com o valor da instância\n",
    "    min_val, max_val = valor_original, valor_original\n",
    "    \n",
    "    # Perturba negativamente\n",
    "    for _ in range(max_iter):\n",
    "        min_val -= passo\n",
    "        if min_val < min_val_data:\n",
    "            min_val = min_val_data\n",
    "            break\n",
    "        instancia_perturbada = instancia.copy()\n",
    "        instancia_perturbada[feature] = min_val\n",
    "        predicao = modelo.predict(instancia_perturbada)\n",
    "        if predicao[0] != classe_desejada:\n",
    "            min_val += passo\n",
    "            break\n",
    "\n",
    "    # Perturba positivamente\n",
    "    for _ in range(max_iter):\n",
    "        max_val += passo\n",
    "        if max_val > max_val_data:\n",
    "            max_val = max_val_data\n",
    "            break\n",
    "        instancia_perturbada = instancia.copy()\n",
    "        instancia_perturbada[feature] = max_val\n",
    "        predicao = modelo.predict(instancia_perturbada)\n",
    "        if predicao[0] != classe_desejada:\n",
    "            max_val -= passo\n",
    "            break\n",
    "\n",
    "    return min_val, max_val\n",
    "\n",
    "# Exemplo de uso (substitua X e y pelos dados adequados)\n",
    "analisar_instancias(X, y, class_names, classe_0=0)\n",
    "# Crie um dicionário para armazenar a contagem de cada feature\n",
    "contagem_features = {}\n",
    "\n",
    "# Itere sobre cada item da lista TUDO\n",
    "for item in TUDO:\n",
    "    # Verifique se o item é uma lista\n",
    "    if isinstance(item, list):\n",
    "        # Itere sobre cada item da lista\n",
    "        for feature in item:\n",
    "            # Extraia o nome da feature\n",
    "            nome_feature = feature.split(\" - \")[0]\n",
    "\n",
    "            # Verifique se a feature já está no dicionário\n",
    "            if nome_feature in contagem_features:\n",
    "                # Incremente a contagem\n",
    "                contagem_features[nome_feature] += 1\n",
    "            else:\n",
    "                # Adicione a feature ao dicionário com contagem 1\n",
    "                contagem_features[nome_feature] = 1\n",
    "\n",
    "# Imprima quantidade em que as features aparecem na explicacão em ordem\n",
    "#for nome_feature, contagem in contagem_features.items():\n",
    "#    print(f\"Feature: {nome_feature} Contagem: {contagem}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature: worst area Contagem: 114\n",
      "Feature: mean perimeter Contagem: 113\n",
      "Feature: worst radius Contagem: 112\n",
      "Feature: mean radius Contagem: 112\n",
      "Feature: worst perimeter Contagem: 106\n",
      "Feature: worst texture Contagem: 105\n",
      "Feature: area error Contagem: 104\n",
      "Feature: texture error Contagem: 95\n",
      "Feature: mean area Contagem: 89\n",
      "Feature: mean texture Contagem: 81\n",
      "Feature: worst concavity Contagem: 76\n",
      "Feature: perimeter error Contagem: 74\n",
      "Feature: worst compactness Contagem: 74\n",
      "Feature: worst symmetry Contagem: 74\n",
      "Feature: worst concave points Contagem: 74\n",
      "Feature: mean concavity Contagem: 74\n",
      "Feature: mean compactness Contagem: 74\n",
      "Feature: worst smoothness Contagem: 74\n",
      "Feature: mean symmetry Contagem: 74\n",
      "Feature: mean concave points Contagem: 74\n",
      "Feature: mean smoothness Contagem: 74\n",
      "Feature: worst fractal dimension Contagem: 74\n",
      "Feature: concavity error Contagem: 74\n",
      "Feature: radius error Contagem: 74\n",
      "Feature: compactness error Contagem: 74\n",
      "Feature: mean fractal dimension Contagem: 74\n",
      "Feature: symmetry error Contagem: 74\n",
      "Feature: concave points error Contagem: 74\n",
      "Feature: smoothness error Contagem: 74\n",
      "Feature: fractal dimension error Contagem: 74\n"
     ]
    }
   ],
   "source": [
    "# Imprima quantidade em que as features aparecem na explicacão em ordem\n",
    "ordenando = sorted(contagem_features.items(), key=lambda item: item[1], reverse=True)\n",
    "for nome_feature, contagem in ordenando:\n",
    "    print(f\"Feature: {nome_feature} Contagem: {contagem}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
