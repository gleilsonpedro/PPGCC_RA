{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pesos: [[1.37515913 3.50507158]]\n",
      "Intercepto: [-9.9889242]\n",
      "Acurácia: 1.00\n"
     ]
    }
   ],
   "source": [
    "####################################################################################################\n",
    "import pandas as pd\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Carrega o dataset Iris\n",
    "iris = load_iris()\n",
    "df = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df['target'] = iris.target #rotulos\n",
    "df = df[['sepal length (cm)', 'petal width (cm)', 'target']] # 3 colunas\n",
    "df = df[df['target'].isin([0, 1])] # seleciona calsses setosa e versicolor\n",
    "# separando as clounas classes 0 e 1 e features\n",
    "R = [0, 1] \n",
    "X = df.iloc[:, R]  \n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "modelo = LogisticRegression()\n",
    "modelo.fit(X_train, y_train)\n",
    "score = modelo.score(X_test, y_test)\n",
    "\n",
    "w1 = modelo.coef_[0][0]  # Peso'sepal length'\n",
    "w2 = modelo.coef_[0][1]  # Peso 'petal width'\n",
    "wo = modelo.intercept_[0]  # Termo constante gerado pela reg. logistica\n",
    "\n",
    "print(f\"Pesos: {modelo.coef_}\")\n",
    "print(f\"Intercepto: {modelo.intercept_}\")\n",
    "print(f\"Acurácia: {score:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classe: 0\n",
      "PI-explicação: {'(1, [4.9, 0.2])'}\n",
      "  - Sepal Length (peso - 1.3751591278961863): 4.9 cm\n",
      "    Valor Mínimo para Sepal Length: 4.3 cm\n",
      "  - Petal Width (peso - 3.505071575134543): 0.2 cm\n",
      "    Valor Mínimo para Petal Width: 0.1 cm\n",
      "Classe: 1\n",
      "PI-explicação: {'(1, [4.9, 0.2])'}\n",
      "  - Sepal Length (peso - 1.3751591278961863): 4.9 cm\n",
      "    Valor Mínimo para Sepal Length: 4.3 cm\n",
      "  - Petal Width (peso - 3.505071575134543): 0.2 cm\n",
      "    Valor Mínimo para Petal Width: 0.1 cm\n"
     ]
    }
   ],
   "source": [
    "########## FUNÇÕES ##########\n",
    "def ONEEXPLANATION(Vs, delta, R, Idx, Xpl, classe):\n",
    "    \"\"\"\n",
    "    Encontrar uma PI-explicação usando um algoritmo guloso.\n",
    "    Args:\n",
    "        Vs: Valores da instância.\n",
    "        delta: Lista ordenada de valores de delta.\n",
    "        R: Limite de explicação.\n",
    "        Idx: Índice atual na lista delta.\n",
    "        Xpl: Conjunto de literais da explicação.\n",
    "        classe: Classe atual sendo considerada.\n",
    "    Returns:\n",
    "        Tupla com o limite atualizado (R) e o índice atualizado (Idx).\n",
    "    \"\"\"\n",
    "    # Verifica se Idx está dentro dos limites da lista antes de incrementá-lo\n",
    "    if Idx + 1 < len(delta):\n",
    "        # Se o peso da primeira feature for maior\n",
    "        if abs(w1) > abs(w2):\n",
    "            Idx = 0 # Escolhe a primeira feature como a mais importante\n",
    "        else:\n",
    "            Idx = 1 # Escolhe a segunda feature como a mais importante\n",
    "\n",
    "        R -= delta[Idx]\n",
    "        # Converte a tupla para string para que seja \"hashable\"\n",
    "        Xpl.add(str((Idx, Vs[Idx])))  # Adiciona o literal à PI-explicação\n",
    "        EXPLICAR_PI(Xpl, classe, w1, w2, df)  # Imprime ou processa a PI-explicação atual\n",
    "        return R, Idx\n",
    "    else:\n",
    "        # Se Idx estiver fora dos limites, interrompe o loop\n",
    "        return R, Idx\n",
    "\n",
    "def ALLEXPLANATIONS(Vs, delta, threshold, w1, w2):\n",
    "    \"\"\"\n",
    "    Enumerar todas as PI-explicações usando backtracking, considerando todas as classes.\n",
    "\n",
    "    Args:\n",
    "        Vs: Valores da instância.\n",
    "        delta: Lista ordenada de valores de delta.\n",
    "        threshold: Limite de explicação.\n",
    "        w1: Peso para a primeira feature.\n",
    "        w2: Peso para a segunda feature.\n",
    "\n",
    "    Returns:\n",
    "        Lista de tuplas com a classe e a PI-explicação para cada instância.\n",
    "    \"\"\"\n",
    "    pi_explicacoes = []  # Lista para armazenar as PI-explicações\n",
    "    for classe in range(2):  # Itera sobre as duas classes (0 e 1)\n",
    "        Xpl = set()  # cria um set vazio para ramazenar os literais da pi-explica\n",
    "        Idx = 0\n",
    "        R = 0\n",
    "        \n",
    "        while Idx >= 0 and Idx < len(delta): \n",
    "            R, Idx = ONEEXPLANATION(Vs, delta, R, Idx, Xpl, classe)\n",
    "            # Ajuste para garantir que R não se torne negativo\n",
    "            if R < 0:\n",
    "                R = 0\n",
    "            # Adiciona a PI-explicação à lista, apenas se não existir na lista\n",
    "            if (classe, Xpl) not in pi_explicacoes:\n",
    "                pi_explicacoes.append((classe, Xpl))  # Adiciona a PI-explicação à lista\n",
    "            Idx += 1 # Incrementa Idx após a chamada da função ONEEXPLANATION\n",
    "    return pi_explicacoes\n",
    "# print(pi_explicacoes)\n",
    "# print(delta)\n",
    "def EXPLICAR_PI(Xpl, classe, w1, w2, df):\n",
    "    \"\"\"Imprime a PI-explicação.\"\"\"\n",
    "    print(f\"Classe: {classe}\")\n",
    "    print(f\"PI-explicação: {Xpl}\")\n",
    "    for item in Xpl:\n",
    "         idx, valores = eval(item)\n",
    "         if idx == 0:\n",
    "             print(f\"  - Sepal Length ({w1}): {valores[0]} cm\")\n",
    "             print(f\"    Valor Mínimo para Sepal Length: {df['sepal length (cm)'].min()} cm\")\n",
    "             print(f\"  - Petal Width ({w2}): {valores[1]} cm\")\n",
    "             print(f\"    Valor Mínimo para Petal Width: {df['petal width (cm)'].min()} cm\")\n",
    "         else:\n",
    "             print(f\"  - Sepal Length (peso - {w1}): {valores[0]} cm\")\n",
    "             print(f\"    Valor Mínimo para Sepal Length: {df['sepal length (cm)'].min()} cm\")\n",
    "             print(f\"  - Petal Width (peso - {w2}): {valores[1]} cm\")\n",
    "             print(f\"    Valor Mínimo para Petal Width: {df['petal width (cm)'].min()} cm\")\n",
    "\n",
    "delta = []\n",
    "for feature in df.columns[:-1]:  # Exclui a ultima coluna 'target'\n",
    "    delta_feature = df[feature].max() - df[feature].min()\n",
    "    delta.append(delta_feature)\n",
    "\n",
    "# com o(threshold) mais baixo o classificador pode ficar mais rigoroso\n",
    "threshold = 0 \n",
    "\n",
    "Vs = []\n",
    "for index, row in df.iterrows():\n",
    "    Vs.append(list(row[:-1]))  # adiciona os pares Sepal e petal\n",
    "\n",
    "pi_explicacoes = ALLEXPLANATIONS(Vs, delta, threshold, w1, w2)\n",
    "\n",
    "# Cria um DataFrame com as PI-explicações\n",
    "pi_explicacoes_df = pd.DataFrame(pi_explicacoes, columns=['Classe', 'PI-Explicação'])\n",
    "\n",
    "# print(f'\\n{pi_explicacoes_df}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
